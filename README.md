# [[Date 7/23]]
트랜스포머 중 BERT와 RoBERT 모델을 사용하기 위해 gitHub huggingface의 modeling코드를 살펴봄

## [BERT의 중요 아키텍쳐]
1. Self-Attention Mechanism : 입력 문장의 각 단어가 다른 단어와의 관계를 학습할 수 있도록 한다. 이를 통해 문장의 각 단어가 다른 모든 단어와의 상호작용을 한다.
관련 함수: 줄 222
2. Positional Encodinig: 입력 시퀀스의 단어 순서를 인식하기 위해 사용된다. 각 단어의 위치 정보를 추가한다.
3. Bidirection Context(양방향 컨텍스트): 문장을 한 방향이 아닌, 왼쪽 오른쪽 양방향으로 읽는다.

## [BERT의 두 단계 학습 과정]
1. pre-training 대량의 텍스트 데이터에서 모델을 사전 훈련한다.
   - Masked Language Model(MLM): 문장에서 일부 단어를 무작위로 마스킹하고, 모델이 이 단어들을 예측하도록 한다.
   - Next Sentence Prediction(NSP): 두 문장이 주어졌을 때, 두 번째 첫 번째 문장의 다음 문장인지 여부를 예측한다.
2. Fine-tuning 특정 작업(감정 분석, 질문 응답)으로 모델을 조정한다.

## "우리가 해야 할 작업"
1. 트랜스포머로 "욕설이 있지만 긍정인 문장"과 "욕설은 없지만 부정인 문장"을 분별할 수 있는 모델을 구성하는 것.
2. "시발점"과 같은 욕설의 단어가 포함되었다고 오해될 수 있는 단어들을 먼저 분류하기 위한 일반 단어 사전 데이터 셋 모으기
